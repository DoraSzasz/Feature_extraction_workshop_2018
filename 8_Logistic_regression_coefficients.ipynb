{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Coefficients\n",
    "\n",
    "Linear regression is a straightforward approach for predicting a quantitative response Y on the basis of a different predictor variable X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we can write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\n",
    "\n",
    "**The magnitude of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients across features, it is importance to have all features within the same scale. This is why, normalisation is important for variable importance and feature selection in linear models. Normalisation is important as well for model performance.\n",
    "\n",
    "In addition, Linear Regression makes the following assumptions over the predictor variables X:\n",
    "- Linear relationship with the outcome Y\n",
    "- Multivariate normality (X should follow a Gaussian distribution)\n",
    "- No or little multicollinearity (Xs should not be linearly related to one another)\n",
    "- Homoscedasticity (variance should be the same)\n",
    "\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
    "\n",
    "Therefore, there are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
    "\n",
    "In addition, these coefficients may be penalised by regularisation, therefore being smaller than if we were to compare only that individual feature with the target.\n",
    "\n",
    "Having said this, you can still select features based on linear regression coefficients, provided you keep all of these in mind at the time of analysing the outcome.\n",
    "\n",
    "Personally, this is not our feature selection method of choice, although we find it useful to interpret the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel # helps us select the most relevant features\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 132)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('training_data/paribas_test.csv', nrows=50000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.375465e+00</td>\n",
       "      <td>11.361141</td>\n",
       "      <td>C</td>\n",
       "      <td>4.200778</td>\n",
       "      <td>6.57700</td>\n",
       "      <td>2.081784</td>\n",
       "      <td>1.784386</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>...</td>\n",
       "      <td>7.619048</td>\n",
       "      <td>1.815241</td>\n",
       "      <td>1.112270e-07</td>\n",
       "      <td>AF</td>\n",
       "      <td>1.292368</td>\n",
       "      <td>3.903345</td>\n",
       "      <td>1.485925</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333334</td>\n",
       "      <td>1.428572e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-4.903407e-07</td>\n",
       "      <td>8.201529</td>\n",
       "      <td>C</td>\n",
       "      <td>4.544371</td>\n",
       "      <td>6.55010</td>\n",
       "      <td>1.558442</td>\n",
       "      <td>2.467532</td>\n",
       "      <td>0.007164</td>\n",
       "      <td>7.142858</td>\n",
       "      <td>...</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>1.970928</td>\n",
       "      <td>1.412265e-02</td>\n",
       "      <td>AV</td>\n",
       "      <td>1.128724</td>\n",
       "      <td>5.844156</td>\n",
       "      <td>1.475892</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263157</td>\n",
       "      <td>-6.380022e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2.661870e+00</td>\n",
       "      <td>3.041241</td>\n",
       "      <td>C</td>\n",
       "      <td>1.657216</td>\n",
       "      <td>9.77308</td>\n",
       "      <td>2.078337</td>\n",
       "      <td>1.430855</td>\n",
       "      <td>1.252157</td>\n",
       "      <td>7.959596</td>\n",
       "      <td>...</td>\n",
       "      <td>4.404040</td>\n",
       "      <td>8.163614</td>\n",
       "      <td>1.100329e+00</td>\n",
       "      <td>B</td>\n",
       "      <td>1.988688</td>\n",
       "      <td>1.558753</td>\n",
       "      <td>2.448814</td>\n",
       "      <td>0</td>\n",
       "      <td>5.385474</td>\n",
       "      <td>1.493777e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1.252822e+00</td>\n",
       "      <td>11.283352</td>\n",
       "      <td>C</td>\n",
       "      <td>4.638388</td>\n",
       "      <td>8.52051</td>\n",
       "      <td>2.302484</td>\n",
       "      <td>3.510159</td>\n",
       "      <td>0.074263</td>\n",
       "      <td>7.612904</td>\n",
       "      <td>...</td>\n",
       "      <td>6.580644</td>\n",
       "      <td>1.325654</td>\n",
       "      <td>2.584588e-01</td>\n",
       "      <td>A</td>\n",
       "      <td>1.863796</td>\n",
       "      <td>2.666478</td>\n",
       "      <td>2.374275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681672</td>\n",
       "      <td>2.264151e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID            v1         v2 v3        v4       v5        v6        v7  \\\n",
       "0   0  1.375465e+00  11.361141  C  4.200778  6.57700  2.081784  1.784386   \n",
       "1   1           NaN        NaN  C       NaN      NaN       NaN       NaN   \n",
       "2   2 -4.903407e-07   8.201529  C  4.544371  6.55010  1.558442  2.467532   \n",
       "3   7  2.661870e+00   3.041241  C  1.657216  9.77308  2.078337  1.430855   \n",
       "4  10  1.252822e+00  11.283352  C  4.638388  8.52051  2.302484  3.510159   \n",
       "\n",
       "         v8        v9      ...           v122      v123          v124  v125  \\\n",
       "0  0.011094  9.523810      ...       7.619048  1.815241  1.112270e-07    AF   \n",
       "1       NaN       NaN      ...            NaN       NaN           NaN     I   \n",
       "2  0.007164  7.142858      ...       5.714286  1.970928  1.412265e-02    AV   \n",
       "3  1.252157  7.959596      ...       4.404040  8.163614  1.100329e+00     B   \n",
       "4  0.074263  7.612904      ...       6.580644  1.325654  2.584588e-01     A   \n",
       "\n",
       "       v126      v127      v128  v129      v130          v131  \n",
       "0  1.292368  3.903345  1.485925     0  2.333334  1.428572e+00  \n",
       "1       NaN       NaN       NaN     0       NaN           NaN  \n",
       "2  1.128724  5.844156  1.475892     0  1.263157 -6.380022e-07  \n",
       "3  1.988688  1.558753  2.448814     0  5.385474  1.493777e+00  \n",
       "4  1.863796  2.666478  2.374275     0  0.681672  2.264151e+00  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 113)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity we will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"labels ['target'] not contained in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-edd7d50ef4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# separate train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3108\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3138\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4388\u001b[0;31m                     'labels %s not contained in axis' % labels[mask])\n\u001b[0m\u001b[1;32m   4389\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"labels ['target'] not contained in axis\""
     ]
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cd82e791f1d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to scale the data, as the magnitude of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0;31m# is important to make a fair comparison among the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler() # to scale the data, as the magnitude of the data \n",
    "                    # is important to make a fair comparison among the variables\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we will do the model fitting and feature selection\n",
    "# altogether in one line of code\n",
    "\n",
    "# first we specify the Logistic Regression model, here we\n",
    "# select the Ridge Penalty (l2)(it is the default parameter in sklearn)\n",
    "\n",
    "# remember that here we want to evaluate the coefficient magnitud\n",
    "# itself and not whether lasso shrinks coefficients to zero\n",
    "\n",
    "# ideally, we want to avoid regularisation at all, so the coefficients\n",
    "# are not affected (modified) by the penalty of the regularisation\n",
    "\n",
    "# In order to do this in sklearn, we set the parameter C really by\n",
    "# this is basically like fitting a non regularised logistic regression\n",
    "\n",
    "# Then we use the selectFromModel object from sklearn\n",
    "# to automatically select the features\n",
    "\n",
    "# set C to 1000, to avoid regularisation\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1000, penalty='l2')) \n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False,  True,\n",
       "        True,  True, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False,  True, False, False,  True,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True, False,  True, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False,  True, False, False,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True,  True, False, False, False, False, False, False,  True,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were kept.\n",
    "\n",
    "# sklearn will select those features which coefficients are greater\n",
    "# than the mean of all the coefficients.\n",
    "\n",
    "# it compares absolute values of coefficients. More on this in a second.\n",
    "\n",
    "sel_.get_support()  # indicates the selected features with True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "# and then let's sum the number of selected features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of features which coefficient was shrank to zero\n",
    "# is zero, as expected, because we are not making a regularised \n",
    "# regression (I set C too big, which is inversely proportional\n",
    "# to the penaly).\n",
    "\n",
    "np.sum(sel_.estimator_.coef_ == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0073899507415845821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as select from model selects coefficients above the mean\n",
    "# of all coefficients, let's calculate first the mean\n",
    "\n",
    "sel_.estimator_.coef_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11c93ce80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the distribution of coefficients\n",
    "\n",
    "pd.Series(sel_.estimator_.coef_.ravel()).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, some coefficients are positive and some are negative, suggesting that some features are negatively associated with the outcome (the more of the feature the less of the outcome) and viceversa.\n",
    "\n",
    "However, the absolute value of the coefficients inform about the importance of the feature on the outcome, and not the sign. Therefore, the feature selection is done filtering on absolute values of coefficients. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.046355710686889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feature importance is informed by the absolute value of\n",
    "# the coefficient, and not the sign.\n",
    "# therefore, let's recalculate the mean using the absolute values instead\n",
    "\n",
    "np.abs(sel_.estimator_.coef_).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11a4de438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the histogram of absolute coefficients\n",
    "\n",
    "pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 112\n",
      "selected features: 29\n",
      "features with coefficients greater than the mean coefficient: 29\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which coefficient is above the\n",
    "# mean coefficient, to make sure we understand the output of\n",
    "# sklearn\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients greater than the mean coefficient: {}'.format(\n",
    "    np.sum(np.abs(sel_.estimator_.coef_) > np.abs(sel_.estimator_.coef_).mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we see how select from model works. It will select all the coefficients which absolute values are greater than the mean.\n",
    "You can of course select a different threshold. Visit the documentation in sklearn to learn how to change this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
